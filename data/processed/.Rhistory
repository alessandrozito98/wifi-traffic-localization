df1 <- df1[,sort(names(df1))]
df2 <- df2[,sort(names(df2))]
# All of this allows us to use rbind
return(rbind(df1,df2))
}
# For AY.103
ay103 <- read.csv("Documents/GitHub/AppliedStatisticsProject2022/data/lineages/AY.103/binmatrix_ay103.csv")
ay103shuffle <- ay103[sample(nrow(ay103)),]
ay103shuffle <- ay103[sample(nrow(ay103)),]
ay103_topk <- ay103shuffle[1:10000,-c(2,3)]
rownames(ay103_topk) <- NULL
remove(ay103, ay103shuffle)
gc()
View(ay103_topk)
View(ay103_topk)
binmatrix <- ay103_topk
View(binmatrix)
forpc <- binmatrix[,-1]
pc <- princomp(forpc, score =T)
summary(pc)
distance_mat <- dist(ay103_clust, method = 'euclidean')
# We perform hierarchical clustering with Ward Linkage
ay103_clust <- ay103_topk[,-1]
distance_mat <- dist(ay103_clust, method = 'euclidean')
Hierar_cl <- hclust(distance_mat, method = "ward.D")
fit <- cutree(Hierar_cl, k = 5 )
# Do we see any special characteristics in the clusters?
# We plot the mutation frequencies barplots
mutation_frequencies_bis <- function(binmatrix) {
freq <-  as.data.frame(matrix(nrow=ncol(binmatrix),ncol=2))
colnames(freq) <- c("mutation", "frequency")
for(i in 1:ncol(binmatrix)) {
freq[i,1] <- colnames(binmatrix)[i]
freq[i,2] <- sum(binmatrix[,i] == 1)/nrow(binmatrix)
}
return(freq)
}
# Now let's color them based on the clustering
fit <- cutree(Hierar_cl, k = 5)
freq <- mutation_frequencies_bis(forpc)
freq <- freq[order(freq$frequency, decreasing = T),]
X11()
par(mar=c(10,5,10,5))
barplot(height = freq$frequency, names = freq$mutation, las = 2, ylab="mutation percentage", main="Barplot of mutation frequencies in AY.103")
cluster1 <- forpc[fit == 1,]
cluster1
freq1 <- mutation_frequencies_bis(cluster1)
freq1 <- freq1[match(freq$mutation, freq1$mutation),]
#freq1 <- freq1[order(freq1$frequency, decreasing = T),]
X11()
par(mar=c(10,5,10,5))
barplot(height = freq1$frequency, names = freq1$mutation, las = 2, ylab="mutation percentage", main="Barplot cluster 1 in AY.103")
cluster1 <- forpc[fit == 2,]
cluster1
freq1 <- mutation_frequencies_bis(cluster1)
freq1 <- freq1[match(freq$mutation, freq1$mutation),]
par(mar=c(10,5,10,5))
barplot(height = freq1$frequency, names = freq1$mutation, las = 2, ylab="mutation percentage", main="Barplot cluster 2 in AY.103")
#freq1 <- freq1[order(freq1$frequency, decreasing = T),]
X11()
par(mar=c(10,5,10,5), mfrow=c(1,2))
barplot(height = freq$frequency, names = freq$mutation, las = 2, ylab="mutation percentage", main="Barplot of mutation frequencies in AY.103")
cluster1 <- forpc[fit == 2,]
cluster1
freq1 <- mutation_frequencies_bis(cluster1)
freq1 <- freq1[match(freq$mutation, freq1$mutation),]
barplot(height = freq1$frequency, names = freq1$mutation, las = 2, ylab="mutation percentage", main="Barplot cluster 2 in AY.103")
q()
setwd("~/Documents/GitHub/second_capture/processed")
setwd("~/Documents/GitHub/wifi-traffic-localization/data/second_capture/processed")
#browsing <- read.csv("processed_browsing.csv", stringsAsFactors = T)
#idle <- read.csv("processed_idle.csv", stringsAsFactors = T)
#instagram <- read.csv("processed_instagram.csv", stringsAsFactors = T)
#netflix <- read.csv("processed_netflix.csv", stringsAsFactors = T)
#spotify <- read.csv("processed_spotify.csv", stringsAsFactors = T)
videocall <- read.csv("processed_videocall.csv", stringsAsFactors = T)
voip <- read.csv("processed_voip.csv", stringsAsFactors = T)
youtube <- read.csv("processed_youtube.csv", stringsAsFactors = T)
# This is the final dataset
#dataset <- rbind(browsing, idle, instagram, netflix, spotify, videocall, voip, youtube)
dataset <- rbind(videocall, voip, youtube)
# We look at the boxplot per column to study the variance
notype <- dataset[,1:11]
X11()
boxplot(notype, col = 'yellow')
#################################################################
#         Training our classifier
#################################################################
# Next step -> shuffle data and extract 70% for testing
shuffled <- dataset[sample(1:nrow(dataset)),]
train <- shuffled[1:(floor(0.65*nrow(shuffled))),]
test_traffic <- shuffled[(nrow(train) + 1):nrow(shuffled),]
# Normalize training data
notype <- scale(train[,1:11])
# We compute the principal components and also the scores
pc <- princomp(notype, scores = T)
summary(pc)
X11()
pairs(pc$scores[,1:3], col=train$type_of_traffic, pch = 19)
legend("topright", fill = unique(train$type_of_traffic), legend = c(levels(train$type_of_traffic)))
# Leave-one-out cross validation on training set to choose optimal parameter
errors <- vector(mode="numeric", length = 49)
for(k in 2:50) {
for(i in 1:nrow(notype)) {
trainset <- notype[-i,]
testset <- notype[i,]
train_target <- train$type_of_traffic[-i]
traffic.knn <- knn(train = trainset, test = testset, cl = train_target, k = k)
if(traffic.knn[1] != train$type_of_traffic[i])
errors[k-1] <- errors[k-1] + 1
}
}
library(car)
library(rgl)
library(mvtnorm)
library(MASS)
library(class)
library(e1071)
# Leave-one-out cross validation on training set to choose optimal parameter
errors <- vector(mode="numeric", length = 49)
for(k in 2:50) {
for(i in 1:nrow(notype)) {
trainset <- notype[-i,]
testset <- notype[i,]
train_target <- train$type_of_traffic[-i]
traffic.knn <- knn(train = trainset, test = testset, cl = train_target, k = k)
if(traffic.knn[1] != train$type_of_traffic[i])
errors[k-1] <- errors[k-1] + 1
}
}
min(errors)
errors
# We scale testing dataset
test_scaled <- scale(test_traffic[,1:11], attr(notype, "scaled:center"), attr(notype, "scaled:scale"))
# We "train" KNN
traffic.knn <- knn(train = notype, test = test_scaled, cl = train$type_of_traffic, k = 4)
traffic.knn
# Confusion table
table(class.true = test$type_of_traffic, class.assigned=traffic.knn)
test_traffic <- shuffled[(nrow(train) + 1):nrow(shuffled),]
# Confusion table
table(class.true = test_traffic$type_of_traffic, class.assigned=traffic.knn)
# We "train" KNN
traffic.knn <- knn(train = notype, test = test_scaled, cl = train$type_of_traffic, k = 50)
traffic.knn
# Confusion table
table(class.true = test_traffic$type_of_traffic, class.assigned=traffic.knn)
x11()
# We "train" KNN
traffic.knn <- knn(train = notype, test = test_scaled, cl = train$type_of_traffic, k = 100)
traffic.knn
# Confusion table
table(class.true = test_traffic$type_of_traffic, class.assigned=traffic.knn)
# We "train" KNN
traffic.knn <- knn(train = notype, test = test_scaled, cl = train$type_of_traffic, k = 200)
# We "train" KNN
traffic.knn <- knn(train = notype, test = test_scaled, cl = train$type_of_traffic, k = 170)
traffic.knn
# Confusion table
table(class.true = test_traffic$type_of_traffic, class.assigned=traffic.knn)
# We "train" KNN
traffic.knn <- knn(train = notype, test = test_scaled, cl = train$type_of_traffic, k = 4)
traffic.knn
# Confusion table
table(class.true = test_traffic$type_of_traffic, class.assigned=traffic.knn)
x11()
# Confusion table
table(class.true = test_traffic$type_of_traffic, class.assigned=traffic.knn)
x11()
plot(pc$scores[,1:2], main='Traffic', pch=19, col = train$type_of_traffic)
legend("topright", fill = unique(train$type_of_traffic), legend = c(levels(train$type_of_traffic)))
colnames(dataset)
notype$mean_pl
notype[,1]
View(notype)
colnames(dataset)
mean_pl  <- seq(min(notype$mean_pl), max(max(notype$mean_pl)), length=300)
noptype[,1]
notype[,1]
min(notype[,1])
mean_pl  <- seq(min(notype[,1]), max(max(notype[,1]), length=300)
mean_pl  <- seq(min(notype[,1]), max(max(notype[,1]), length=300))
mean_pl  <- seq(min(notype[,1]), max(max(notype[,1]), length=300)
mean_pl  <- seq(min(notype[,1]), max(notype[,1]), length=300)
mean_pl
colnames(dataset)
mean_pl  <- seq(min(notype[,1]), max(notype[,1]), length=300)
var_pl  <- seq(min(notype[,2]), max(notype[,2]), length=300)
mean_iartime  <- seq(min(notype[,3]), max(notype[,3]), length=300)
var_ieartime  <- seq(min(notype[,4]), max(notype[,4]), length=300)
max_length  <- seq(min(notype[,5]), max(notype[,5]), length=300)
min_length  <- seq(min(notype[,6]), max(notype[,6]), length=300)
num_qosd  <- seq(min(notype[,7]), max(notype[,7]), length=300)
num_qosnull  <- seq(min(notype[,8]), max(notype[,8]), length=300)
num_other  <- seq(min(notype[,9]), max(notype[,9]), length=300)
num_up  <- seq(min(notype[,10]), max(notype[,10]), length=300)
num_down  <- seq(min(notype[,11]), max(notype[,11]), length=300)
xy
xy <- expand.grid(xcoord=x, ycoord=y)
y <- c(1,3,4,6,7)
x <- c(1,4,5,6,7)
xy <- expand.grid(xcoord=x, ycoord=y)
xy
plot(pc$scores[,1:2], main='Traffic', pch=19, col = train$type_of_traffic)
legend("topright", fill = unique(train$type_of_traffic), legend = c(levels(train$type_of_traffic)))
# Scree plot (plot of explained variance per component)
X11()
plot(cumsum(pc$sd^2)/sum(pc$sd^2), type='b', axes=F, xlab='number of components',
ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(dataset),labels=1:ncol(dataset),las=2)
pc$scores
pc$scores[,1:3]
scores_1_3 <- pc$scores[,1:3]
pc1  <- seq(min(scores_1_3[,1]), max(scores_1_3[,1]), length=300)
pc2  <- seq(min(scores_1_3[,2]), max(scores_1_3[,2]), length=300)
pc3  <- seq(min(scores_1_3[,3]), max(scores_1_3[,3]), length=300)
xy <- expand.grid(xcoord=pc1, ycoord=pc2)
xy <- expand.grid(xcoord=pc1, ycoord=pc2, zcoord=pc3)
xyz <- expand.grid(xcoord=pc1, ycoord=pc2, zcoord=pc3)
remove(xy)
gc()
?expand.grid
train
debris.knn <- knn(train = notype, test = xyz, cl = train$type_of_traffic, k = 4)
contour.knn <- knn(train = scores_1_3, test = xyz, cl = train$type_of_traffic, k = 4)
z  <- as.numeric(contour.knn)
z
x11()
plot(pc$scores[,1:2], main='Traffic', pch=19, col = train$type_of_traffic)
legend("topright", fill = unique(train$type_of_traffic), legend = c(levels(train$type_of_traffic)))
contour(pc1, pc2, matrix(z, 300), levels=c(1.5, 2.5), drawlabels=F, add=T)
pc1  <- seq(min(scores_1_3[,1]), max(scores_1_3[,1]), length=300)
pc2  <- seq(min(scores_1_3[,2]), max(scores_1_3[,2]), length=300)
#xyz <- expand.grid(xcoord=pc1, ycoord=pc2, zcoord=pc3)
xy <- expand.grid(xcoord=pc1, ycoord=pc2)
contour.knn <- knn(train = scores_1_3[,1:2], test = xy, cl = train$type_of_traffic, k = 4)
z  <- as.numeric(contour.knn)
contour(pc1, pc2, matrix(z, 300), levels=c(1.5, 2.5), drawlabels=F, add=T)
remove(xyz)
remove(xy)
gc()
pc1  <- seq(min(scores_1_3[,1]), max(scores_1_3[,1]), length=300)
pc2  <- seq(min(scores_1_3[,2]), max(scores_1_3[,2]), length=300)
pc3  <- seq(min(scores_1_3[,3]), max(scores_1_3[,3]), length=300)
xyz <- expand.grid(xcoord=pc1, ycoord=pc2, zcoord=pc3)
contour.knn <- knn(train = scores_1_3[,1:3], test = xyz, cl = train$type_of_traffic, k = 4)
z  <- as.numeric(contour.knn)
z
contour(pc1, pc2, matrix(z, 27000000), levels=c(1.5, 2.5), drawlabels=F, add=T)
?matrix
contour(pc1, pc2, matrix(z, 27000000), levels=c(1.5, 2.5), drawlabels=F, add=T)
z
length(z)
#xyz <- expand.grid(xcoord=pc1, ycoord=pc2, zcoord=pc3)
xy <- expand.grid(x = pc1, y = pc2)
contour.knn <- knn(train = scores_1_3[,1:2], test = xy, cl = train$type_of_traffic, k = 4)
z  <- as.numeric(contour.knn)
contour(pc1, pc2, matrix(z, 300), levels=c(1.5, 2.5), drawlabels=F, add=T)
traffic <- read.csv("webbrowsing.csv", stringsAsFactors = T)
setwd("~/Documents/wireless_project/second_capture")
traffic <- read.csv("webbrowsing.csv", stringsAsFactors = T)
setwd("~/Documents/wireless_project/second_capture")
getwd()
setwd("~/Documents/GitHub/wifi-traffic-localization/data/second_capture/")
traffic <- read.csv("webbrowsing.csv", stringsAsFactors = T)
# Filter non-802.11 packets
traffic <- subset(traffic, traffic$Protocol == "802.11")
unique(traffic$Protocol)
# Initialize time
epoch <- traffic$Time[1]
end_of_time <- traffic$Time[nrow(traffic)]
interval <- 15
# compute number of rows
tot_time <- end_of_time - epoch
traffic_rows <- floor(tot_time/interval)
# Create empty dataframe with column names
dataset <- data.frame(matrix(ncol = 12, nrow = traffic_rows))
names <- c("mean_pl", "var_pl", "mean_iartime", "var_iartime",
"max_length", "min_length", "num_qosd", "num_qosnull",
"num_other", "num_up", "num_down", "type_of_traffic")
colnames(dataset) <- names
dataset$type_of_traffic <- "Web browsing"
start_interval <- epoch
for(i in 1:traffic_rows) {
traffic_interval <- subset(traffic, traffic$Time >= start_interval & traffic$Time < start_interval + interval)
nrows <- nrow(traffic_interval)
if(nrows != 0) {
dataset[i,]$mean_pl <- mean(traffic_interval$Length)
dataset[i,]$var_pl <- var(traffic_interval$Length)
dataset[i,]$max_length <- max(traffic_interval$Length)
dataset[i,]$min_length <- min(traffic_interval$Length)
qosdata_rows <- nrow(traffic_interval[grep('QoS Data', traffic_interval$Info),])
qosnull_rows <-  nrow(traffic_interval[grep('QoS Null function', traffic_interval$Info),])
other_rows <- nrows - qosdata_rows - qosnull_rows
dataset[i,]$num_qosd <- qosdata_rows
dataset[i,]$num_qosnull <- qosnull_rows
dataset[i,]$num_other <- other_rows
rec_rows <- nrow(traffic_interval[grep('a4:42:3b:d2:f7:08|IntelCor_d2:f7:08', traffic_interval$Destination),])
sent_rows <- nrow(traffic_interval[grep('a4:42:3b:d2:f7:08|IntelCor_d2:f7:08', traffic_interval$Source),])
dataset[i,]$num_down <- rec_rows
dataset[i,]$num_up <- sent_rows
if(nrows != 1) {
iar <- vector(mode = "numeric", length = (nrows - 1))
for(j in 1:(nrows - 1))
iar[j] <- traffic_interval$Time[j + 1] - traffic_interval$Time[j]
dataset[i,]$mean_iartime <- mean(iar)
dataset[i,]$var_iartime <- var(iar)
if(length(iar) < 2)
dataset[i,]$var_iartime <- 0
}
else {
dataset[i,]$var_pl <- 0
dataset[i,]$mean_iartime <- 0
dataset[i,]$var_iartime <- 0
}
}
else
dataset[i,1:12] <- 0
start_interval <- start_interval + interval
}
write.csv(dataset, "processed/processed_webbrowsing.csv", row.names = FALSE)
View(dataset)
traffic <- read.csv("idle.csv", stringsAsFactors = T)
# Filter non-802.11 packets
traffic <- subset(traffic, traffic$Protocol == "802.11")
unique(traffic$Protocol)
# Initialize time
epoch <- traffic$Time[1]
end_of_time <- traffic$Time[nrow(traffic)]
interval <- 15
# compute number of rows
tot_time <- end_of_time - epoch
traffic_rows <- floor(tot_time/interval)
# Create empty dataframe with column names
dataset <- data.frame(matrix(ncol = 12, nrow = traffic_rows))
names <- c("mean_pl", "var_pl", "mean_iartime", "var_iartime",
"max_length", "min_length", "num_qosd", "num_qosnull",
"num_other", "num_up", "num_down", "type_of_traffic")
colnames(dataset) <- names
dataset$type_of_traffic <- "Idle"
start_interval <- epoch
for(i in 1:traffic_rows) {
traffic_interval <- subset(traffic, traffic$Time >= start_interval & traffic$Time < start_interval + interval)
nrows <- nrow(traffic_interval)
if(nrows != 0) {
dataset[i,]$mean_pl <- mean(traffic_interval$Length)
dataset[i,]$var_pl <- var(traffic_interval$Length)
dataset[i,]$max_length <- max(traffic_interval$Length)
dataset[i,]$min_length <- min(traffic_interval$Length)
qosdata_rows <- nrow(traffic_interval[grep('QoS Data', traffic_interval$Info),])
qosnull_rows <-  nrow(traffic_interval[grep('QoS Null function', traffic_interval$Info),])
other_rows <- nrows - qosdata_rows - qosnull_rows
dataset[i,]$num_qosd <- qosdata_rows
dataset[i,]$num_qosnull <- qosnull_rows
dataset[i,]$num_other <- other_rows
rec_rows <- nrow(traffic_interval[grep('a4:42:3b:d2:f7:08|IntelCor_d2:f7:08', traffic_interval$Destination),])
sent_rows <- nrow(traffic_interval[grep('a4:42:3b:d2:f7:08|IntelCor_d2:f7:08', traffic_interval$Source),])
dataset[i,]$num_down <- rec_rows
dataset[i,]$num_up <- sent_rows
if(nrows != 1) {
iar <- vector(mode = "numeric", length = (nrows - 1))
for(j in 1:(nrows - 1))
iar[j] <- traffic_interval$Time[j + 1] - traffic_interval$Time[j]
dataset[i,]$mean_iartime <- mean(iar)
dataset[i,]$var_iartime <- var(iar)
if(length(iar) < 2)
dataset[i,]$var_iartime <- 0
}
else {
dataset[i,]$var_pl <- 0
dataset[i,]$mean_iartime <- 0
dataset[i,]$var_iartime <- 0
}
}
else
dataset[i,1:12] <- 0
start_interval <- start_interval + interval
}
write.csv(dataset, "processed/processed_idle.csv", row.names = FALSE)
View(dataset)
remove(list = ls())
gc()
library(car)
library(rgl)
library(mvtnorm)
library(MASS)
library(class)
library(e1071)
setwd("~/Documents/GitHub/wifi-traffic-localization/data/second_capture/processed")
browsing <- read.csv("processed_webbrowsing.csv", stringsAsFactors = T)
idle <- read.csv("processed_idle.csv", stringsAsFactors = T)
#instagram <- read.csv("processed_instagram.csv", stringsAsFactors = T)
#netflix <- read.csv("processed_netflix.csv", stringsAsFactors = T)
#spotify <- read.csv("processed_spotify.csv", stringsAsFactors = T)
videocall <- read.csv("processed_videocall.csv", stringsAsFactors = T)
voip <- read.csv("processed_voip.csv", stringsAsFactors = T)
youtube <- read.csv("processed_youtube.csv", stringsAsFactors = T)
# This is the final dataset
#dataset <- rbind(browsing, idle, instagram, netflix, spotify, videocall, voip, youtube)
dataset <- rbind(videocall, voip, youtube, browsing, idle)
# We look at the boxplot per column to study the variance
notype <- dataset[,1:11]
X11()
boxplot(notype, col = 'yellow')
unique(dataset$type_of_traffic)
# This is the final dataset
#dataset <- rbind(browsing, idle, instagram, netflix, spotify, videocall, voip, youtube)
dataset <- rbind(videocall, voip, youtube, browsing, idle)
# We look at the boxplot per column to study the variance
notype <- dataset[,1:11]
# The data isn't centered, we center the data around its mean
notype <- scale(notype, scale = FALSE)
boxplot(notype, col = 'yellow')
# We standardize the data because of the variance of one of the features
notype <- scale(notype)
boxplot(notype, col = 'yellow')
X11()
boxplot(notype, col = 'yellow')
# The data isn't centered, we center the data around its mean
notype <- scale(notype, scale = FALSE)
boxplot(notype, col = 'yellow')
# We standardize the data because of the variance of one of the features
notype <- scale(notype)
boxplot(notype, col = 'yellow')
# We standardize the data because of the variance of one of the features
notype <- scale(notype)
# We standardize the data because of the variance of one of the features
notype <- scale(notype)
boxplot(notype, col = 'yellow')
# We look at the boxplot per column to study the variance
notype <- dataset[,1:11]
boxplot(notype, col = 'yellow')
# The data isn't centered, we center the data around its mean
notype <- scale(notype, scale = FALSE)
boxplot(notype, col = 'yellow')
# We standardize the data because of the variance of one of the features
notype <- scale(notype)
boxplot(notype, col = 'yellow')
# We compute the principal components and also the scores
pc <- princomp(notype, scores = T)
summary(pc)
# Scree plot (plot of explained variance per component)
X11()
plot(cumsum(pc$sd^2)/sum(pc$sd^2), type='b', axes=F, xlab='number of components',
ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(dataset),labels=1:ncol(dataset),las=2)
x11()
par(mfcol = c(2,2))
for(i in 1:4) barplot(pc$loadings[,i], ylim = c(-1, 1), main=paste("PC",i), col = "yellow")
X11()
plot(pc$scores[,1:2], col=dataset$type_of_traffic, pch=19)
X11()
pairs(pc$scores[,1:3], col=dataset$type_of_traffic, pch = 19)
legend("topright", fill = unique(dataset$type_of_traffic), legend = c(levels(dataset$type_of_traffic)))
#################################################################
#         Training our classifier
#################################################################
# Next step -> shuffle data and extract 70% for testing
shuffled <- dataset[sample(1:nrow(dataset)),]
train <- shuffled[1:(floor(0.65*nrow(shuffled))),]
test_traffic <- shuffled[(nrow(train) + 1):nrow(shuffled),]
# Normalize training data
notype <- scale(train[,1:11])
X11()
boxplot(notype, col = 'yellow')
# We compute the principal components and also the scores
pc <- princomp(notype, scores = T)
summary(pc)
X11()
pairs(pc$scores[,1:3], col=train$type_of_traffic, pch = 19)
legend("topright", fill = unique(train$type_of_traffic), legend = c(levels(train$type_of_traffic)))
# Leave-one-out cross validation on training set to choose optimal parameter
errors <- vector(mode="numeric", length = 49)
for(k in 2:50) {
for(i in 1:nrow(notype)) {
trainset <- notype[-i,]
testset <- notype[i,]
train_target <- train$type_of_traffic[-i]
traffic.knn <- knn(train = trainset, test = testset, cl = train_target, k = k)
if(traffic.knn[1] != train$type_of_traffic[i])
errors[k-1] <- errors[k-1] + 1
}
}
min(errors)
errors
# We scale testing dataset
test_scaled <- scale(test_traffic[,1:11], attr(notype, "scaled:center"), attr(notype, "scaled:scale"))
# We "train" KNN
traffic.knn <- knn(train = notype, test = test_scaled, cl = train$type_of_traffic, k = 4)
traffic.knn
# Confusion table
table(class.true = test_traffic$type_of_traffic, class.assigned=traffic.knn)
x11()
plot(pc$scores[,1:2], main='Traffic', pch=19, col = train$type_of_traffic)
legend("topright", fill = unique(train$type_of_traffic), legend = c(levels(train$type_of_traffic)))
scores_1_3 <- pc$scores[,1:3]
pc1  <- seq(min(scores_1_3[,1]), max(scores_1_3[,1]), length=300)
pc2  <- seq(min(scores_1_3[,2]), max(scores_1_3[,2]), length=300)
#xyz <- expand.grid(xcoord=pc1, ycoord=pc2, zcoord=pc3)
xy <- expand.grid(x = pc1, y = pc2)
contour.knn <- knn(train = scores_1_3[,1:2], test = xy, cl = train$type_of_traffic, k = 4)
z  <- as.numeric(contour.knn)
contour(pc1, pc2, matrix(z, 300), levels=c(1.5, 2.5), drawlabels=F, add=T)
12/167
